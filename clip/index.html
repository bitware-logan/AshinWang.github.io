<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.ashin.wang","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"appID":"S30ZCDT4BR","apiKey":"6988e9b27ba9570d9ed85020ebf48b01","indexName":"blog","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="CLIP图文多模态对比学习">
<meta property="og:url" content="https://www.ashin.wang/clip/index.html">
<meta property="og:site_name" content="Ashin Wang&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwai3xfuhpj31o20l6whp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwaixo2ypgj312o0kwjst.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwai8kacltj31340u0adf.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwaikbq9i7j30u40u0423.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwb41oxur0j30gc0cmt9a.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L=max(0,%7C%7Cy_%7Ba%7D-y_%7Bp%7D%7C%7C_%7B2%7D%5E%7B2%7D+-+%7C%7Cy_%7Ba%7D-y_%7Bn%7D%7C%7C_%7B2%7D%5E%7B2%7D+%5Calpha)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7C%7Cy_%7Ba%7D-y_%7Bp%7D%7C%7C_%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwaj8g8h5aj30f006pdgu.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bf%7D_%7B%5Cmathrm%7Bimg%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bf%7D_%7B%5Cmathrm%7Btext%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_%7Be%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7BL_i%7D+=++-+%5Clog+(%7Be%5E%7BS(%7Bz_i%7D,z_i%5E+++)/%5Ctau+%7D%7D/%5Csum%5Cnolimits_%7Bj+=+0%7D%5EK+%7B%7Be%5E%7BS(%7Bz_i%7D,%7Bz_j%7D)/%5Ctau+%7D%7D%7D+)+">
<meta property="og:image" content="https://pic4.zhimg.com/v2-5a46a1feba4f6e92cc944cbc5b57389f_b.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwbe8uuc0vj31400i9ab7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwbefbjwakj311r0u0n3f.jpg">
<meta property="og:image" content="https://github.com/orpatashnik/StyleCLIP/raw/main/img/teaser.png">
<meta property="article:published_time" content="2021-11-10T15:59:15.000Z">
<meta property="article:modified_time" content="2021-11-11T11:08:02.928Z">
<meta property="article:author" content="Ashin Wang">
<meta property="article:tag" content="CLIP">
<meta property="article:tag" content="搜索">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="对比学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/008i3skNgy1gwai3xfuhpj31o20l6whp.jpg">

<link rel="canonical" href="https://www.ashin.wang/clip/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>CLIP图文多模态对比学习 | Ashin Wang's Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?1d0c251d59c914b8a773d2fd3302cc56";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ashin Wang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.ashin.wang/clip/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g74lvmm1zhj3074074q4r.jpg">
      <meta itemprop="name" content="Ashin Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashin Wang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CLIP图文多模态对比学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-10 23:59:15" itemprop="dateCreated datePublished" datetime="2021-11-10T23:59:15+08:00">2021-11-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">对比学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwai3xfuhpj31o20l6whp.jpg" style="zoom: 50%;" />

<span id="more"></span>

<h1 id="CLIP简介"><a href="#CLIP简介" class="headerlink" title="CLIP简介"></a>CLIP简介</h1><p>Repo: <a target="_blank" rel="noopener" href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a></p>
<p><strong>CLIP (Contrastive Language-Image Pre-Training)</strong> is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet “zero-shot” without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.</p>
<h1 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h1><h2 id="官方"><a href="#官方" class="headerlink" title="官方"></a>官方</h2><h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple, Union</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># all conv layers have stride 1. an avgpool is performed after the second convolution when stride &gt; 1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AvgPool2d(stride) <span class="keyword">if</span> stride &gt; <span class="number">1</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * self.expansion, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = <span class="literal">None</span></span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride &gt; <span class="number">1</span> <span class="keyword">or</span> inplanes != planes * Bottleneck.expansion:</span><br><span class="line">            <span class="comment"># downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1</span></span><br><span class="line">            self.downsample = nn.Sequential(OrderedDict([</span><br><span class="line">                (<span class="string">&quot;-1&quot;</span>, nn.AvgPool2d(stride)),</span><br><span class="line">                (<span class="string">&quot;0&quot;</span>, nn.Conv2d(inplanes, planes * self.expansion, <span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)),</span><br><span class="line">                (<span class="string">&quot;1&quot;</span>, nn.BatchNorm2d(planes * self.expansion))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.relu(self.bn2(self.conv2(out)))</span><br><span class="line">        out = self.avgpool(out)</span><br><span class="line">        out = self.bn3(self.conv3(out))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionPool2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, spacial_dim: <span class="built_in">int</span>, embed_dim: <span class="built_in">int</span>, num_heads: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span> = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** <span class="number">2</span> + <span class="number">1</span>, embed_dim) / embed_dim ** <span class="number">0.5</span>)</span><br><span class="line">        self.k_proj = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        self.q_proj = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        self.v_proj = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        self.c_proj = nn.Linear(embed_dim, output_dim <span class="keyword">or</span> embed_dim)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.reshape(x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>] * x.shape[<span class="number">3</span>]).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; (HW)NC</span></span><br><span class="line">        x = torch.cat([x.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>), x], dim=<span class="number">0</span>)  <span class="comment"># (HW+1)NC</span></span><br><span class="line">        x = x + self.positional_embedding[:, <span class="literal">None</span>, :].to(x.dtype)  <span class="comment"># (HW+1)NC</span></span><br><span class="line">        x, _ = F.multi_head_attention_forward(</span><br><span class="line">            query=x, key=x, value=x,</span><br><span class="line">            embed_dim_to_check=x.shape[-<span class="number">1</span>],</span><br><span class="line">            num_heads=self.num_heads,</span><br><span class="line">            q_proj_weight=self.q_proj.weight,</span><br><span class="line">            k_proj_weight=self.k_proj.weight,</span><br><span class="line">            v_proj_weight=self.v_proj.weight,</span><br><span class="line">            in_proj_weight=<span class="literal">None</span>,</span><br><span class="line">            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),</span><br><span class="line">            bias_k=<span class="literal">None</span>,</span><br><span class="line">            bias_v=<span class="literal">None</span>,</span><br><span class="line">            add_zero_attn=<span class="literal">False</span>,</span><br><span class="line">            dropout_p=<span class="number">0</span>,</span><br><span class="line">            out_proj_weight=self.c_proj.weight,</span><br><span class="line">            out_proj_bias=self.c_proj.bias,</span><br><span class="line">            use_separate_proj_weight=<span class="literal">True</span>,</span><br><span class="line">            training=self.training,</span><br><span class="line">            need_weights=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModifiedResNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A ResNet class that is similar to torchvision&#x27;s but contains the following changes:</span></span><br><span class="line"><span class="string">    - There are now 3 &quot;stem&quot; convolutions as opposed to 1, with an average pool instead of a max pool.</span></span><br><span class="line"><span class="string">    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1</span></span><br><span class="line"><span class="string">    - The final pooling layer is a QKV attention instead of an average pool</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layers, output_dim, heads, input_resolution=<span class="number">224</span>, width=<span class="number">64</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line"></span><br><span class="line">        <span class="comment"># the 3-layer stem</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, width // <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(width // <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(width // <span class="number">2</span>, width // <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(width // <span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(width // <span class="number">2</span>, width, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(width)</span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># residual layers</span></span><br><span class="line">        self._inplanes = width  <span class="comment"># this is a *mutable* variable used during construction</span></span><br><span class="line">        self.layer1 = self._make_layer(width, layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(width * <span class="number">2</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(width * <span class="number">4</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(width * <span class="number">8</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        embed_dim = width * <span class="number">32</span>  <span class="comment"># the ResNet feature dimension</span></span><br><span class="line">        self.attnpool = AttentionPool2d(input_resolution // <span class="number">32</span>, embed_dim, heads, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span>(<span class="params">self, planes, blocks, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        layers = [Bottleneck(self._inplanes, planes, stride)]</span><br><span class="line"></span><br><span class="line">        self._inplanes = planes * Bottleneck.expansion</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(Bottleneck(self._inplanes, planes))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">stem</span>(<span class="params">x</span>):</span></span><br><span class="line">            <span class="keyword">for</span> conv, bn <span class="keyword">in</span> [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:</span><br><span class="line">                x = self.relu(bn(conv(x)))</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = x.<span class="built_in">type</span>(self.conv1.weight.dtype)</span><br><span class="line">        x = stem(x)</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        x = self.attnpool(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.LayerNorm</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Subclass torch&#x27;s LayerNorm to handle fp16.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        orig_type = x.dtype</span><br><span class="line">        ret = <span class="built_in">super</span>().forward(x.<span class="built_in">type</span>(torch.float32))</span><br><span class="line">        <span class="keyword">return</span> ret.<span class="built_in">type</span>(orig_type)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuickGELU</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.sigmoid(<span class="number">1.702</span> * x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualAttentionBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, n_head: <span class="built_in">int</span>, attn_mask: torch.Tensor = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.attn = nn.MultiheadAttention(d_model, n_head)</span><br><span class="line">        self.ln_1 = LayerNorm(d_model)</span><br><span class="line">        self.mlp = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&quot;c_fc&quot;</span>, nn.Linear(d_model, d_model * <span class="number">4</span>)),</span><br><span class="line">            (<span class="string">&quot;gelu&quot;</span>, QuickGELU()),</span><br><span class="line">            (<span class="string">&quot;c_proj&quot;</span>, nn.Linear(d_model * <span class="number">4</span>, d_model))</span><br><span class="line">        ]))</span><br><span class="line">        self.ln_2 = LayerNorm(d_model)</span><br><span class="line">        self.attn_mask = attn_mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) <span class="keyword">if</span> self.attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.attn(x, x, x, need_weights=<span class="literal">False</span>, attn_mask=self.attn_mask)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        x = x + self.attention(self.ln_1(x))</span><br><span class="line">        x = x + self.mlp(self.ln_2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, width: <span class="built_in">int</span>, layers: <span class="built_in">int</span>, heads: <span class="built_in">int</span>, attn_mask: torch.Tensor = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.width = width</span><br><span class="line">        self.layers = layers</span><br><span class="line">        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.resblocks(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VisionTransformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_resolution: <span class="built_in">int</span>, patch_size: <span class="built_in">int</span>, width: <span class="built_in">int</span>, layers: <span class="built_in">int</span>, heads: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        scale = width ** -<span class="number">0.5</span></span><br><span class="line">        self.class_embedding = nn.Parameter(scale * torch.randn(width))</span><br><span class="line">        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** <span class="number">2</span> + <span class="number">1</span>, width))</span><br><span class="line">        self.ln_pre = LayerNorm(width)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(width, layers, heads)</span><br><span class="line"></span><br><span class="line">        self.ln_post = LayerNorm(width)</span><br><span class="line">        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        x = self.conv1(x)  <span class="comment"># shape = [*, width, grid, grid]</span></span><br><span class="line">        x = x.reshape(x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># shape = [*, width, grid ** 2]</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># shape = [*, grid ** 2, width]</span></span><br><span class="line">        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[<span class="number">0</span>], <span class="number">1</span>, x.shape[-<span class="number">1</span>], dtype=x.dtype, device=x.device), x], dim=<span class="number">1</span>)  <span class="comment"># shape = [*, grid ** 2 + 1, width]</span></span><br><span class="line">        x = x + self.positional_embedding.to(x.dtype)</span><br><span class="line">        x = self.ln_pre(x)</span><br><span class="line"></span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># NLD -&gt; LND</span></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># LND -&gt; NLD</span></span><br><span class="line"></span><br><span class="line">        x = self.ln_post(x[:, <span class="number">0</span>, :])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.proj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x @ self.proj</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CLIP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embed_dim: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="comment"># vision</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 image_resolution: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vision_layers: Union[Tuple[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>], <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 vision_width: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vision_patch_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="comment"># text</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 context_length: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 transformer_width: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 transformer_heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 transformer_layers: <span class="built_in">int</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.context_length = context_length</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(vision_layers, (<span class="built_in">tuple</span>, <span class="built_in">list</span>)):</span><br><span class="line">            vision_heads = vision_width * <span class="number">32</span> // <span class="number">64</span></span><br><span class="line">            self.visual = ModifiedResNet(</span><br><span class="line">                layers=vision_layers,</span><br><span class="line">                output_dim=embed_dim,</span><br><span class="line">                heads=vision_heads,</span><br><span class="line">                input_resolution=image_resolution,</span><br><span class="line">                width=vision_width</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vision_heads = vision_width // <span class="number">64</span></span><br><span class="line">            self.visual = VisionTransformer(</span><br><span class="line">                input_resolution=image_resolution,</span><br><span class="line">                patch_size=vision_patch_size,</span><br><span class="line">                width=vision_width,</span><br><span class="line">                layers=vision_layers,</span><br><span class="line">                heads=vision_heads,</span><br><span class="line">                output_dim=embed_dim</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(</span><br><span class="line">            width=transformer_width,</span><br><span class="line">            layers=transformer_layers,</span><br><span class="line">            heads=transformer_heads,</span><br><span class="line">            attn_mask=self.build_attention_mask()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, transformer_width)</span><br><span class="line">        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))</span><br><span class="line">        self.ln_final = LayerNorm(transformer_width)</span><br><span class="line"></span><br><span class="line">        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))</span><br><span class="line">        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(<span class="number">1</span> / <span class="number">0.07</span>))</span><br><span class="line"></span><br><span class="line">        self.initialize_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        nn.init.normal_(self.token_embedding.weight, std=<span class="number">0.02</span>)</span><br><span class="line">        nn.init.normal_(self.positional_embedding, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self.visual, ModifiedResNet):</span><br><span class="line">            <span class="keyword">if</span> self.visual.attnpool <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                std = self.visual.attnpool.c_proj.in_features ** -<span class="number">0.5</span></span><br><span class="line">                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)</span><br><span class="line">                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)</span><br><span class="line">                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)</span><br><span class="line">                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> resnet_block <span class="keyword">in</span> [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:</span><br><span class="line">                <span class="keyword">for</span> name, param <span class="keyword">in</span> resnet_block.named_parameters():</span><br><span class="line">                    <span class="keyword">if</span> name.endswith(<span class="string">&quot;bn3.weight&quot;</span>):</span><br><span class="line">                        nn.init.zeros_(param)</span><br><span class="line"></span><br><span class="line">        proj_std = (self.transformer.width ** -<span class="number">0.5</span>) * ((<span class="number">2</span> * self.transformer.layers) ** -<span class="number">0.5</span>)</span><br><span class="line">        attn_std = self.transformer.width ** -<span class="number">0.5</span></span><br><span class="line">        fc_std = (<span class="number">2</span> * self.transformer.width) ** -<span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.transformer.resblocks:</span><br><span class="line">            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)</span><br><span class="line">            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)</span><br><span class="line">            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)</span><br><span class="line">            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.text_projection <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.normal_(self.text_projection, std=self.transformer.width ** -<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_attention_mask</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># lazily create causal attention mask, with full attention between the vision tokens</span></span><br><span class="line">        <span class="comment"># pytorch uses additive attention mask; fill with -inf</span></span><br><span class="line">        mask = torch.empty(self.context_length, self.context_length)</span><br><span class="line">        mask.fill_(<span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">        mask.triu_(<span class="number">1</span>)  <span class="comment"># zero out the lower diagonal</span></span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dtype</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.visual.conv1.weight.dtype</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode_image</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.visual(image.<span class="built_in">type</span>(self.dtype))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode_text</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        x = self.token_embedding(text).<span class="built_in">type</span>(self.dtype)  <span class="comment"># [batch_size, n_ctx, d_model]</span></span><br><span class="line"></span><br><span class="line">        x = x + self.positional_embedding.<span class="built_in">type</span>(self.dtype)</span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># NLD -&gt; LND</span></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># LND -&gt; NLD</span></span><br><span class="line">        x = self.ln_final(x).<span class="built_in">type</span>(self.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x.shape = [batch_size, n_ctx, transformer.width]</span></span><br><span class="line">        <span class="comment"># take features from the eot embedding (eot_token is the highest number in each sequence)</span></span><br><span class="line">        x = x[torch.arange(x.shape[<span class="number">0</span>]), text.argmax(dim=-<span class="number">1</span>)] @ self.text_projection</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, image, text</span>):</span></span><br><span class="line">        image_features = self.encode_image(image)</span><br><span class="line">        text_features = self.encode_text(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># normalized features</span></span><br><span class="line">        image_features = image_features / image_features.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        text_features = text_features / text_features.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cosine similarity as logits</span></span><br><span class="line">        logit_scale = self.logit_scale.exp()</span><br><span class="line">        logits_per_image = logit_scale * image_features @ text_features.t()</span><br><span class="line">        logits_per_text = logits_per_image.t()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape = [global_batch_size, global_batch_size]</span></span><br><span class="line">        <span class="keyword">return</span> logits_per_image, logits_per_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_weights</span>(<span class="params">model: nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert applicable model parameters to fp16&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_weights_to_fp16</span>(<span class="params">l</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):</span><br><span class="line">            l.weight.data = l.weight.data.half()</span><br><span class="line">            <span class="keyword">if</span> l.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                l.bias.data = l.bias.data.half()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(l, nn.MultiheadAttention):</span><br><span class="line">            <span class="keyword">for</span> attr <span class="keyword">in</span> [*[<span class="string">f&quot;<span class="subst">&#123;s&#125;</span>_proj_weight&quot;</span> <span class="keyword">for</span> s <span class="keyword">in</span> [<span class="string">&quot;in&quot;</span>, <span class="string">&quot;q&quot;</span>, <span class="string">&quot;k&quot;</span>, <span class="string">&quot;v&quot;</span>]], <span class="string">&quot;in_proj_bias&quot;</span>, <span class="string">&quot;bias_k&quot;</span>, <span class="string">&quot;bias_v&quot;</span>]:</span><br><span class="line">                tensor = <span class="built_in">getattr</span>(l, attr)</span><br><span class="line">                <span class="keyword">if</span> tensor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    tensor.data = tensor.data.half()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&quot;text_projection&quot;</span>, <span class="string">&quot;proj&quot;</span>]:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(l, name):</span><br><span class="line">                attr = <span class="built_in">getattr</span>(l, name)</span><br><span class="line">                <span class="keyword">if</span> attr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    attr.data = attr.data.half()</span><br><span class="line"></span><br><span class="line">    model.apply(_convert_weights_to_fp16)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">state_dict: <span class="built_in">dict</span></span>):</span></span><br><span class="line">    vit = <span class="string">&quot;visual.proj&quot;</span> <span class="keyword">in</span> state_dict</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vit:</span><br><span class="line">        vision_width = state_dict[<span class="string">&quot;visual.conv1.weight&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">        vision_layers = <span class="built_in">len</span>([k <span class="keyword">for</span> k <span class="keyword">in</span> state_dict.keys() <span class="keyword">if</span> k.startswith(<span class="string">&quot;visual.&quot;</span>) <span class="keyword">and</span> k.endswith(<span class="string">&quot;.attn.in_proj_weight&quot;</span>)])</span><br><span class="line">        vision_patch_size = state_dict[<span class="string">&quot;visual.conv1.weight&quot;</span>].shape[-<span class="number">1</span>]</span><br><span class="line">        grid_size = <span class="built_in">round</span>((state_dict[<span class="string">&quot;visual.positional_embedding&quot;</span>].shape[<span class="number">0</span>] - <span class="number">1</span>) ** <span class="number">0.5</span>)</span><br><span class="line">        image_resolution = vision_patch_size * grid_size</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        counts: <span class="built_in">list</span> = [<span class="built_in">len</span>(<span class="built_in">set</span>(k.split(<span class="string">&quot;.&quot;</span>)[<span class="number">2</span>] <span class="keyword">for</span> k <span class="keyword">in</span> state_dict <span class="keyword">if</span> k.startswith(<span class="string">f&quot;visual.layer<span class="subst">&#123;b&#125;</span>&quot;</span>))) <span class="keyword">for</span> b <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">        vision_layers = <span class="built_in">tuple</span>(counts)</span><br><span class="line">        vision_width = state_dict[<span class="string">&quot;visual.layer1.0.conv1.weight&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">        output_width = <span class="built_in">round</span>((state_dict[<span class="string">&quot;visual.attnpool.positional_embedding&quot;</span>].shape[<span class="number">0</span>] - <span class="number">1</span>) ** <span class="number">0.5</span>)</span><br><span class="line">        vision_patch_size = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">assert</span> output_width ** <span class="number">2</span> + <span class="number">1</span> == state_dict[<span class="string">&quot;visual.attnpool.positional_embedding&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">        image_resolution = output_width * <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    embed_dim = state_dict[<span class="string">&quot;text_projection&quot;</span>].shape[<span class="number">1</span>]</span><br><span class="line">    context_length = state_dict[<span class="string">&quot;positional_embedding&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">    vocab_size = state_dict[<span class="string">&quot;token_embedding.weight&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">    transformer_width = state_dict[<span class="string">&quot;ln_final.weight&quot;</span>].shape[<span class="number">0</span>]</span><br><span class="line">    transformer_heads = transformer_width // <span class="number">64</span></span><br><span class="line">    transformer_layers = <span class="built_in">len</span>(<span class="built_in">set</span>(k.split(<span class="string">&quot;.&quot;</span>)[<span class="number">2</span>] <span class="keyword">for</span> k <span class="keyword">in</span> state_dict <span class="keyword">if</span> k.startswith(<span class="string">f&quot;transformer.resblocks&quot;</span>)))</span><br><span class="line"></span><br><span class="line">    model = CLIP(</span><br><span class="line">        embed_dim,</span><br><span class="line">        image_resolution, vision_layers, vision_width, vision_patch_size,</span><br><span class="line">        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">&quot;input_resolution&quot;</span>, <span class="string">&quot;context_length&quot;</span>, <span class="string">&quot;vocab_size&quot;</span>]:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> state_dict:</span><br><span class="line">            <span class="keyword">del</span> state_dict[key]</span><br><span class="line"></span><br><span class="line">    convert_weights(model)</span><br><span class="line">    model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>





<h3 id="Calculating-cosine-similarity"><a href="#Calculating-cosine-similarity" class="headerlink" title="Calculating cosine similarity"></a>Calculating cosine similarity</h3><p>Milvus 中的内积 （IP）<a target="_blank" rel="noopener" href="https://milvus.io/cn/docs/v2.0.0/metric.md#floating">https://milvus.io/cn/docs/v2.0.0/metric.md#floating</a></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwaixo2ypgj312o0kwjst.jpg" alt="image-20211111003358477" style="zoom: 50%;" />

<p>We normalize the features and calculate the dot product of each pair.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_features /= image_features.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">text_features /= text_features.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T</span><br></pre></td></tr></table></figure>



<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwai8kacltj31340u0adf.jpg" style="zoom:50%;" />

<h3 id="Zero-Shot-Image-Classification"><a href="#Zero-Shot-Image-Classification" class="headerlink" title="Zero-Shot Image Classification"></a>Zero-Shot Image Classification</h3><ul>
<li><p>CIFAR100 <a target="_blank" rel="noopener" href="https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb">https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb</a></p>
</li>
<li><p>Imagenet <a target="_blank" rel="noopener" href="https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb">https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb</a></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label 转换为 text</span></span><br><span class="line">text_descriptions = [<span class="string">f&quot;This is a photo of a <span class="subst">&#123;label&#125;</span>&quot;</span> <span class="keyword">for</span> label <span class="keyword">in</span> cifar100.classes]</span><br><span class="line">text_tokens = clip.tokenize(text_descriptions).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate features</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># 关闭自动求导引擎，model.eval() 在测试/验证不用 Dropout 表示 dropout=0</span></span><br><span class="line">    text_features = model.encode_text(text_tokens).<span class="built_in">float</span>()</span><br><span class="line">    text_features /= text_features.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can classify images using the cosine similarity (times 100) as the logits to the softmax operation.</span></span><br><span class="line">text_probs = (<span class="number">100.0</span> * image_features @ text_features.T).softmax(dim=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Pick the top 5 most similar labels for the image</span></span><br><span class="line">top_probs, top_labels = text_probs.cpu().topk(<span class="number">5</span>, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwaikbq9i7j30u40u0423.jpg" style="zoom: 67%;" />

<h2 id="非官方"><a href="#非官方" class="headerlink" title="非官方"></a>非官方</h2><h3 id="OpenAI-CLIP-with-train"><a href="#OpenAI-CLIP-with-train" class="headerlink" title="OpenAI CLIP with train"></a><a target="_blank" rel="noopener" href="https://www.kaggle.com/bguberfain/openai-clip-with-train">OpenAI CLIP with train</a></h3><ul>
<li>训练</li>
<li>FAISS</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">_tokenizer = SimpleTokenizer()</span><br><span class="line"><span class="comment"># Copied from https://github.com/openai/CLIP/blob/beba48f35392a73c6c47ae67ddffced81ad1916d/clip/clip.py#L164</span></span><br><span class="line"><span class="comment"># but with relaxed exception</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">texts, context_length: <span class="built_in">int</span> = <span class="number">77</span></span>) -&gt; torch.LongTensor:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(texts, <span class="built_in">str</span>):</span><br><span class="line">        texts = [texts]</span><br><span class="line"></span><br><span class="line">    sot_token = _tokenizer.encoder[<span class="string">&quot;&lt;|startoftext|&gt;&quot;</span>]</span><br><span class="line">    eot_token = _tokenizer.encoder[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>]</span><br><span class="line">    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">    result = torch.zeros(<span class="built_in">len</span>(all_tokens), context_length, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, tokens <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_tokens):</span><br><span class="line">        n = <span class="built_in">min</span>(<span class="built_in">len</span>(tokens), context_length)</span><br><span class="line">        result[i, :n] = torch.tensor(tokens)[:n]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens) &gt; context_length:</span><br><span class="line">            result[i, -<span class="number">1</span>] = tokens[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove EMOJI</span></span><br><span class="line"><span class="comment">## compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。</span></span><br><span class="line">RE_EMOJI = re.<span class="built_in">compile</span>(<span class="string">r&quot;\\x[A-Za-z0-9./]+&quot;</span>, flags=re.UNICODE)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip_emoji</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> RE_EMOJI.sub(<span class="string">r&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RollingMean</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.n = <span class="number">0</span></span><br><span class="line">        self.mean = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.mean = (self.mean * self.n + value) / (self.n + <span class="number">1</span>)</span><br><span class="line">        self.n += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.mean</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Sampler and dataset</span></span><br><span class="line"><span class="string">We implement a sampler that ensures that in every batch, two samples of the same group are always present.</span></span><br><span class="line"><span class="string">This is important in order to use Triplet SemiHardLoss (I&#x27;m using this implementation)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SameGroupSampler</span>(<span class="params">Sampler</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df ,ds</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(ds)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a dictionary of posting_id -&gt; index in dataset</span></span><br><span class="line">        self.index_to_position = <span class="built_in">dict</span>(<span class="built_in">zip</span>(df.index, <span class="built_in">range</span>(<span class="built_in">len</span>(df))))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a Series of label_group -&gt; set(posting_id)</span></span><br><span class="line">        self.label_group = df.reset_index().groupby(<span class="string">&#x27;label_group&#x27;</span>)[<span class="string">&#x27;posting_id&#x27;</span>].apply(<span class="built_in">set</span>).<span class="built_in">map</span>(<span class="built_in">sorted</span>).<span class="built_in">map</span>(np.array)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.label_group)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self)):</span><br><span class="line">            <span class="comment"># Sample one label_group</span></span><br><span class="line">            label_group_sample = self.label_group.sample(<span class="number">1</span>).iloc[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Sample two posting_id&#x27;s</span></span><br><span class="line">            sample1, sample2 = np.random.choice(label_group_sample, <span class="number">2</span>, replace=<span class="literal">False</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">yield</span> self.index_to_position[sample1]</span><br><span class="line">            <span class="keyword">yield</span> self.index_to_position[sample2] </span><br><span class="line">            </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, images_path</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.df = df</span><br><span class="line">        self.images_path = images_path</span><br><span class="line">        self.has_target = (<span class="string">&#x27;label_group&#x27;</span> <span class="keyword">in</span> df)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        row = self.df.iloc[idx]</span><br><span class="line">        </span><br><span class="line">        image = preprocess(Image.<span class="built_in">open</span>(self.images_path / row[<span class="string">&#x27;image&#x27;</span>]))</span><br><span class="line">        text = tokenize([strip_emoji(row[<span class="string">&#x27;title&#x27;</span>])])[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.has_target:</span><br><span class="line">            <span class="keyword">return</span> image, text, row[<span class="string">&#x27;label_group&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> image, text, <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>Finetune CLIP on train data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load CLIP</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">model, preprocess = clip.load(<span class="string">&quot;../input/openai-clip/ViT-B-32.pt&quot;</span>, device=device, jit=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get embedding size</span></span><br><span class="line">embed_dim = model.text_projection.shape[<span class="number">1</span>]</span><br><span class="line">embed_dim</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load train data</span></span><br><span class="line">train_images_path = Path(<span class="string">&#x27;../input/shopee-product-matching/train_images&#x27;</span>)</span><br><span class="line">df_train = pd.read_csv(<span class="string">&#x27;../input/shopee-product-matching/train.csv&#x27;</span>, index_col=<span class="string">&#x27;posting_id&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dstrain = MyDataset(df_train, train_images_path)</span><br><span class="line">dltrain = DataLoader(dstrain, batch_size=<span class="number">128</span>, num_workers=<span class="number">2</span>, sampler=SameGroupSampler(df_train, dstrain))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># optim = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-2)</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.2</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, <span class="number">1e-2</span>, total_steps=n_epochs * (<span class="number">2</span>*<span class="built_in">len</span>(dltrain)-<span class="number">1</span>),</span><br><span class="line">                                               base_momentum=<span class="number">0.0</span>, max_momentum=<span class="number">0.5</span>, pct_start=<span class="number">0.1</span>, div_factor=<span class="number">1e2</span>, final_div_factor=<span class="number">1e4</span>)</span><br><span class="line">criterion = TripletLoss(device)</span><br></pre></td></tr></table></figure>

<p><strong>Triplet loss</strong></p>
<p>Here we use the triplet loss principe to ajust CLIP:</p>
<p>对于三元组 anchor，positive，negative 而言，anchor 为训练集中的一个随机样本，positive 为与 anchor 同类的一个样本，negative 为与 anchor 不同类的一个样本。Tript loss 的作用是最小化 positive 与 anchor 之间的距离，而最大化 negative 与 anchor 之间的距离。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwb41oxur0j30gc0cmt9a.jpg" alt="img"></p>
<p><img src="https://www.zhihu.com/equation?tex=L=max(0,%7C%7Cy_%7Ba%7D-y_%7Bp%7D%7C%7C_%7B2%7D%5E%7B2%7D+-+%7C%7Cy_%7Ba%7D-y_%7Bn%7D%7C%7C_%7B2%7D%5E%7B2%7D+%5Calpha)" alt="[公式]">     <img src="https://www.zhihu.com/equation?tex=%7C%7Cy_%7Ba%7D-y_%7Bp%7D%7C%7C_%7B2%7D" alt="[公式]"> 表示anchor和positive的欧氏距离，<img src="https://www.zhihu.com/equation?tex=%5Calpha+" alt="[公式]"> 表示 positive 和 negative 之间的距离至少为此数。</p>
<ul>
<li>easy triplets （简单三元组）：triplet 对应的损失为 0 的三元组，形式化定义为d(a,n)&gt;d(a,p)+margin，也就是负样本的距离远大于正样本的距离。</li>
<li>hard triplets（困难三元组）：negative example 与 anchor 距离小于 anchor 与 positive example 的距离，形式化定义为 $$d(a,n)&lt;d(a,p)$$，也就是负样本的距离远小于正样本的距离，意味着是易混淆的 case。</li>
<li>semi-hard triplets（一般三元组）：negative example 与 anchor 距离大于 anchor 与 positive example 的距离，但还不至于使得 loss 为0，即 $$d(a,p)&lt;d(a,n)&lt;d(a,p)+margin$$，依旧是介于能区分与容易区分之间，有差距但是差距不够大。负样本的距离虽比正样本大，但不满足间隔裕量margin。此时损失 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]"> 大于 0，但小于margin。</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwaj8g8h5aj30f006pdgu.jpg" alt="Triplet loss"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="number">2</span>*<span class="built_in">len</span>(dltrain)-<span class="number">1</span>) <span class="keyword">as</span> bar:</span><br><span class="line">        loss_mean = RollingMean()</span><br><span class="line">        <span class="keyword">for</span> images, texts, targets <span class="keyword">in</span> dltrain:</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Generate train and text features</span></span><br><span class="line">            images_features = model.encode_image(images.to(device))</span><br><span class="line">            texts_features = model.encode_text(texts.to(device))</span><br><span class="line"></span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Join train and test features</span></span><br><span class="line">            features = torch.hstack([images_features, texts_features])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># L2-normalize features</span></span><br><span class="line">            features = features / features.norm(<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">			</span><br><span class="line">            <span class="comment"># from triplet_loss import TripletLoss</span></span><br><span class="line">            <span class="comment"># criterion = TripletLoss(device)</span></span><br><span class="line">            <span class="comment"># Apply Triplet SemiHardLoss</span></span><br><span class="line">            loss = criterion(features, targets)</span><br><span class="line">			</span><br><span class="line">            loss.backward()</span><br><span class="line">            optim.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update metric and progress bar</span></span><br><span class="line">            loss_mean.update(loss.item())</span><br><span class="line">            bar.update()</span><br><span class="line">            bar.set_description(<span class="string">&#x27;&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_mean.result()))</span><br></pre></td></tr></table></figure>

<p><strong>Run on train</strong></p>
<p>In this section we will generate features using CLIP and perform a similiarity search to find the closest matches.</p>
<p>We create the final set by taking away those results bellow a threshold similiarity (less 0.7)</p>
<p><strong>FAISS</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_similarities_and_indexes</span>(<span class="params">df, images_path, top_n=<span class="number">100</span>, features_file=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># Create pytorch Dataset/DataLoader</span></span><br><span class="line">    ds = MyDataset(df, images_path)</span><br><span class="line">    dl = DataLoader(ds, batch_size=<span class="number">32</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Allocate memory for features</span></span><br><span class="line">    features = np.empty((<span class="built_in">len</span>(df), <span class="number">2</span>*embed_dim), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Begin predict</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, texts, _ <span class="keyword">in</span> tqdm(dl):</span><br><span class="line">        n = <span class="built_in">len</span>(images)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># Generate image and text features</span></span><br><span class="line">            images_features = model.encode_image(images.to(device))</span><br><span class="line">            texts_features = model.encode_text(texts.to(device))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concat features (first images then texts)</span></span><br><span class="line">        features[i:i+n, :embed_dim] = images_features.cpu()</span><br><span class="line">        features[i:i+n, embed_dim:] = texts_features.cpu()</span><br><span class="line"></span><br><span class="line">        i += n</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Option to save these features (may be usefull to tune cut value)</span></span><br><span class="line">    <span class="keyword">if</span> features_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        np.save(features_file, features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># l2-normalize</span></span><br><span class="line">    features /= np.linalg.norm(features, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create index</span></span><br><span class="line">    index = faiss.IndexFlatIP(<span class="number">2</span>*embed_dim)</span><br><span class="line">    index.add(features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Search index</span></span><br><span class="line">    <span class="keyword">return</span> index.search(features, top_n)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> try range_search</span></span><br><span class="line">    <span class="comment"># lims, similarities, indexes = index_test.range_search(test_features, GROUP_CUT)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> RUN_ON_TRAIN:</span><br><span class="line">    <span class="comment"># Perform search of similiar items</span></span><br><span class="line">    similarities, indexes = find_similarities_and_indexes(df_train, train_images_path, features_file=<span class="string">&#x27;features-no-norm.npy&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># `similarities` will have shape (n, 100) and will have the similarites scores for closest matches</span></span><br><span class="line">    <span class="comment"># `indexes` will have shape (n, 100) and have the index closest matches.</span></span><br><span class="line">    <span class="comment"># Both arrays are aligned</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert index to groups, will have shape (n, 100)</span></span><br><span class="line">    found_groups = df_train[<span class="string">&#x27;label_group&#x27;</span>].values[indexes]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check if matches are from same group. Will create a boolean vector of (n, 100)</span></span><br><span class="line">    is_same_group = (found_groups == df_train[<span class="string">&#x27;label_group&#x27;</span>].values[:, np.newaxis])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot similarities score from same group and different groups</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">    plt.hist([similarities[is_same_group], similarities[~is_same_group]], density=<span class="literal">False</span>, bins=<span class="number">51</span>,</span><br><span class="line">         label=[<span class="string">&#x27;Same group&#x27;</span>, <span class="string">&#x27;Different group&#x27;</span>], histtype=<span class="string">&#x27;stepfilled&#x27;</span>, alpha=<span class="number">0.75</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Similarity score&#x27;</span>)</span><br><span class="line">    plt.legend();</span><br></pre></td></tr></table></figure>

<p> <strong>Tune CUT</strong></p>
<p>In this last step we will move the <code>cut_value</code> to find optimal F1-score.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SRC: https://www.kaggle.com/c/shopee-product-matching/discussion/224782#1233338</span></span><br><span class="line"><span class="comment"># With some adaptation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">row_wise_f1_score</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    tp = np.array([<span class="built_in">len</span>(x[<span class="number">0</span>] &amp; x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, y_pred)])</span><br><span class="line">    fp = y_pred.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).values - tp</span><br><span class="line">    fn = y_true.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).values - tp</span><br><span class="line"></span><br><span class="line">    precision = tp / (tp + fp)</span><br><span class="line">    recall = tp / (tp + fn)</span><br><span class="line">    f1 = <span class="number">2</span> * ((precision * recall) / (precision + recall))</span><br><span class="line">    <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_score</span>(<span class="params">cut_value</span>):</span></span><br><span class="line">    <span class="comment"># Apply cutoff of similarities</span></span><br><span class="line">    groups_are_same = (similarities &gt; cut_value)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build results</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i, (group_is_same, index_result) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(groups_are_same, indexes)):</span><br><span class="line">        row_results = df_train.index[index_result[group_is_same]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep found matches as a `set`</span></span><br><span class="line">        results.append(<span class="built_in">set</span>(row_results))</span><br><span class="line"></span><br><span class="line">    df_results = pd.Series(results, index=df_answer.index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Evaluate results</span></span><br><span class="line">    <span class="keyword">return</span> row_wise_f1_score(df_answer, df_results).mean()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> RUN_ON_TRAIN:</span><br><span class="line">    <span class="comment"># Create answer dataframe. This will have posting_id on index and a set of label_group as values </span></span><br><span class="line">    groups = df_train.reset_index().groupby(<span class="string">&#x27;label_group&#x27;</span>)[<span class="string">&#x27;posting_id&#x27;</span>].apply(<span class="built_in">set</span>)</span><br><span class="line">    df_answer = df_train[<span class="string">&#x27;label_group&#x27;</span>].<span class="built_in">map</span>(groups)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cut values to evaluate</span></span><br><span class="line">    cuts = np.linspace(<span class="number">0.5</span>, <span class="number">0.95</span>, <span class="number">51</span>)</span><br><span class="line">    scores = [calc_score(c) <span class="keyword">for</span> c <span class="keyword">in</span> tqdm(cuts)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot curve</span></span><br><span class="line">    plt.plot(cuts, scores)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Cutoff value&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;F1 score&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;Best cutoff is &#123;:.2f&#125; with expected F1 score of &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(cuts[np.argmax(scores)], <span class="built_in">max</span>(scores)))</span><br></pre></td></tr></table></figure>



<h3 id="OpenAI-CLIP-simple-implementation"><a href="#OpenAI-CLIP-simple-implementation" class="headerlink" title="OpenAI CLIP simple implementation"></a><a target="_blank" rel="noopener" href="https://www.kaggle.com/ashinwang/openai-clip-simple-implementation">OpenAI CLIP simple implementation</a></h3><ul>
<li>DistilBert + RN</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ImageEncoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encode images to a fixed size vector</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.model = timm.create_model(</span><br><span class="line">            model_name, pretrained, num_classes=<span class="number">0</span>, global_pool=<span class="string">&quot;avg&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">            p.requires_grad = trainable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.model(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TextEncoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> pretrained:</span><br><span class="line">            self.model = DistilBertModel.from_pretrained(model_name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = DistilBertModel(config=DistilBertConfig())</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">            p.requires_grad = trainable</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we are using the CLS token hidden representation as the sentence&#x27;s embedding</span></span><br><span class="line">        self.target_token_idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span></span><br><span class="line">        output = self.model(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        last_hidden_state = output.last_hidden_state</span><br><span class="line">        <span class="keyword">return</span> last_hidden_state[:, self.target_token_idx, :]</span><br></pre></td></tr></table></figure>

<p><strong>Projetcion Head</strong></p>
<p>通过线性映射层 Projetcion Head 将图片特征<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bf%7D_%7B%5Cmathrm%7Bimg%7D%7D" alt="[公式]">和文本特征<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bf%7D_%7B%5Cmathrm%7Btext%7D%7D" alt="[公式]">都映射到相同的嵌入特征维度<img src="https://www.zhihu.com/equation?tex=D_%7Be%7D" alt="[公式]"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Projetcion Head</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProjectionHead</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        projection_dim=CFG.projection_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">        dropout=CFG.dropout</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Linear(embedding_dim, projection_dim)</span><br><span class="line">        self.gelu = nn.GELU()</span><br><span class="line">        self.fc = nn.Linear(projection_dim, projection_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(projection_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        projected = self.projection(x)</span><br><span class="line">        x = self.gelu(projected)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = x + projected</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p><strong>CLIPModel</strong></p>
<p>直接用文本的 encoding 结果做为图像的监督信号，显然噪声太大了？是否可以早点跟图像特征早点交叉。</p>
<p>采用 <strong>InfoNCE</strong>: info Noise Contrastive Estimation loss  <img src="https://www.zhihu.com/equation?tex=%7BL_i%7D+=++-+%5Clog+(%7Be%5E%7BS(%7Bz_i%7D,z_i%5E+++)/%5Ctau+%7D%7D/%5Csum%5Cnolimits_%7Bj+=+0%7D%5EK+%7B%7Be%5E%7BS(%7Bz_i%7D,%7Bz_j%7D)/%5Ctau+%7D%7D%7D+)+" alt="[公式]">  </p>
<blockquote>
<p>$$t$$ 温度系数的作用是调节对困难样本的关注程度：<strong>越小的温度系数越关注于将本样本和最相似的困难样本分开</strong>，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小。</p>
<p>考虑两个极端情况，温度系数趋向于 0 时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。 </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CLIPModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        temperature=CFG.temperature,</span></span></span><br><span class="line"><span class="function"><span class="params">        image_embedding=CFG.image_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">        text_embedding=CFG.text_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.image_encoder = ImageEncoder()</span><br><span class="line">        self.text_encoder = TextEncoder()</span><br><span class="line">        self.image_projection = ProjectionHead(embedding_dim=image_embedding)</span><br><span class="line">        self.text_projection = ProjectionHead(embedding_dim=text_embedding)</span><br><span class="line">        self.temperature = temperature</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch</span>):</span></span><br><span class="line">        <span class="comment"># Getting Image and Text Features</span></span><br><span class="line">        image_features = self.image_encoder(batch[<span class="string">&quot;image&quot;</span>])</span><br><span class="line">        text_features = self.text_encoder(</span><br><span class="line">            input_ids=batch[<span class="string">&quot;input_ids&quot;</span>], attention_mask=batch[<span class="string">&quot;attention_mask&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Getting Image and Text Embeddings (with same dimension)</span></span><br><span class="line">        image_embeddings = self.image_projection(image_features)</span><br><span class="line">        text_embeddings = self.text_projection(text_features)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculating the Loss</span></span><br><span class="line">        <span class="comment"># Temperature</span></span><br><span class="line">        logits = (text_embeddings @ image_embeddings.T) / self.temperature</span><br><span class="line">        </span><br><span class="line">        images_similarity = image_embeddings @ image_embeddings.T</span><br><span class="line">        texts_similarity = text_embeddings @ text_embeddings.T</span><br><span class="line">        </span><br><span class="line">        targets = F.softmax(</span><br><span class="line">            (images_similarity + texts_similarity) / <span class="number">2</span> * self.temperature, dim=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># cross_entropy</span></span><br><span class="line">        texts_loss = cross_entropy(logits, targets, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        images_loss = cross_entropy(logits.T, targets.T, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        loss =  (images_loss + texts_loss) / <span class="number">2.0</span> <span class="comment"># shape: (batch_size)</span></span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span>(<span class="params">preds, targets, reduction=<span class="string">&#x27;none&#x27;</span></span>):</span></span><br><span class="line">    log_softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">    loss = (-targets * log_softmax(preds)).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> reduction == <span class="string">&quot;none&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    <span class="keyword">elif</span> reduction == <span class="string">&quot;mean&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br></pre></td></tr></table></figure>

<h3 id="CLIP：从自然语言监督中学习可迁移的视觉模型"><a href="#CLIP：从自然语言监督中学习可迁移的视觉模型" class="headerlink" title="CLIP：从自然语言监督中学习可迁移的视觉模型"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/402859811"><em>CLIP</em>：从自然语言监督中学习可迁移的视觉模型</a></h3><img src="https://pic4.zhimg.com/v2-5a46a1feba4f6e92cc944cbc5b57389f_b.jpg" alt="img" style="zoom:67%;" />



<h1 id="CLIP应用"><a href="#CLIP应用" class="headerlink" title="CLIP应用"></a>CLIP应用</h1><h2 id="微博-W-CLIP"><a href="#微博-W-CLIP" class="headerlink" title="微博 W-CLIP"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/370782081">微博 W-CLIP</a></h2><p>对博文-图片来进行表示学习。除了正例构造方法和模型小细节外，博文-图片多模态模型的整体结构和 CLIP 比较接近，所以，我们将这个使用包含大量噪音微博文图数据的多模态模型称为 W-CLIP（Weibo-CLIP）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwbe8uuc0vj31400i9ab7.jpg" alt="img" style="zoom:50%;" />

<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/haltakov/natural-language-image-search">https://github.com/haltakov/natural-language-image-search</a></p>
  <img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwbefbjwakj311r0u0n3f.jpg" style="zoom:50%;" /></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/orpatashnik/StyleCLIP">StyleCLIP</a></p>
</li>
</ul>
<img src="https://github.com/orpatashnik/StyleCLIP/raw/main/img/teaser.png" alt="img" style="zoom:50%;" />

    </div>

    
    
    
      

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Ashin Wang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.ashin.wang/clip/" title="CLIP图文多模态对比学习">https://www.ashin.wang/clip/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CLIP/" rel="tag"># CLIP</a>
              <a href="/tags/%E6%90%9C%E7%B4%A2/" rel="tag"># 搜索</a>
              <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"># 多模态</a>
              <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag"># 对比学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/track-3-of-global-ai-technology-innovation-competition-semantic-matching-of-xiaobu-assistant-dialogue/" rel="prev" title="全球人工智能技术创新大赛赛道三：小布助手对话短文本语义匹配">
      <i class="fa fa-chevron-left"></i> 全球人工智能技术创新大赛赛道三：小布助手对话短文本语义匹配
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CLIP%E7%AE%80%E4%BB%8B"><span class="nav-text">CLIP简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BB%A3%E7%A0%81"><span class="nav-text">关键代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%98%E6%96%B9"><span class="nav-text">官方</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-py"><span class="nav-text">model.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Calculating-cosine-similarity"><span class="nav-text">Calculating cosine similarity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zero-Shot-Image-Classification"><span class="nav-text">Zero-Shot Image Classification</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E5%AE%98%E6%96%B9"><span class="nav-text">非官方</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OpenAI-CLIP-with-train"><span class="nav-text">OpenAI CLIP with train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OpenAI-CLIP-simple-implementation"><span class="nav-text">OpenAI CLIP simple implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CLIP%EF%BC%9A%E4%BB%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%9B%91%E7%9D%A3%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%BF%81%E7%A7%BB%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B"><span class="nav-text">CLIP：从自然语言监督中学习可迁移的视觉模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CLIP%E5%BA%94%E7%94%A8"><span class="nav-text">CLIP应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E5%8D%9A-W-CLIP"><span class="nav-text">微博 W-CLIP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-text">其他</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ashin Wang"
      src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74lvmm1zhj3074074q4r.jpg">
  <p class="site-author-name" itemprop="name">Ashin Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">68</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">89</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AshinWang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AshinWang" rel="noopener" target="_blank"><i class="fa fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ashin Wang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4d1a5c237f1ecba513f0',
      clientSecret: 'f403fbb98b9afbee3a250673d6f741dda22ba1e2',
      repo        : 'AshinWang.github.io',
      owner       : 'AshinWang',
      admin       : ['AshinWang'],
      id          : '74c941f7ca0b71307b223ef1ea34ce43',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
